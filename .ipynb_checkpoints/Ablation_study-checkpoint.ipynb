{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from resnet import ResNet\n",
    "from mahalanobis import compute_centers_cov, mahalanobis_score, metrics_eval\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "ID = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "OOD = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=500, shuffle=False, num_workers=2)\n",
    "ID_loader = torch.utils.data.DataLoader(ID, batch_size=500,shuffle=False,  num_workers=2)\n",
    "OOD_loader = torch.utils.data.DataLoader(OOD, batch_size=500,shuffle=True,  num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Computing Mahalanobis scores when using specific saved ResNet models.\n",
    "\n",
    "# These validation accuracies will be used for the ablation study.\n",
    "# They must correspond to specific trained and saved snapshots of the ResNet model.\n",
    "accuracies = [5733, 6061, 6530, 6929, 7502, 8064, 8587, 8944, 9184]\n",
    "\n",
    "for acc in tqdm(accuracies):\n",
    "    model = torch.load(f'./models/resnet34/cifar10_{acc}')\n",
    "    model.eval()\n",
    "\n",
    "    layers = 17\n",
    "    num_classes = 10\n",
    "    num_per_class = 5000\n",
    "\n",
    "    centers_cov_matrices = [compute_centers_cov(trainloader, model, layer, num_classes, num_per_class)\n",
    "                            for layer in range(0,layers)]\n",
    "    \n",
    "    eps = 0.0000001 # This tiny quantity added to all covariance matrices is here to prevent divisions by zero\n",
    "                    # from happening\n",
    "    try:\n",
    "        inv_cov_matrices = [torch.inverse(S+eps*torch.ones(S.size()).cuda()) for _, S in centers_cov_matrices]\n",
    "    except RuntimeError:\n",
    "        eps = eps * 1.05 # There still may be an error with the first value of eps\n",
    "\n",
    "    # Building score dataset for logistic regression training\n",
    "\n",
    "    magnitude = 0.005 # Chosen from a range of possible values by fine-tuning on last layer results\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    quantity_target = 10000 # No need to compute more scores than this, the loop takes 1 min per batch of\n",
    "                            # 1000 on our machine.\n",
    "    \n",
    "    for data_in, data_out in tqdm(zip(ID_loader,OOD_loader)):\n",
    "        \n",
    "        img_in, img_out = data_in[0].cuda(), data_out[0].cuda()\n",
    "        \n",
    "        features_in = np.zeros((len(img_in),layers))\n",
    "        features_out = np.zeros((len(img_out),layers))\n",
    "        \n",
    "        for layer in range(layers):\n",
    "            \n",
    "            centers, _ = centers_cov_matrices[layer]\n",
    "            inv_cov_matrix = inv_cov_matrices[layer]\n",
    "            \n",
    "            scores_in = mahalanobis_score(img_in, model, centers, inv_cov_matrix, magnitude=magnitude, layer=layer)\n",
    "            scores_out = mahalanobis_score(img_out, model, centers, inv_cov_matrix, magnitude=magnitude, layer=layer)\n",
    "            \n",
    "            features_in[:,layer] = scores_in.cpu().numpy()\n",
    "            features_out[:,layer] = scores_out.cpu().numpy()\n",
    "            \n",
    "        X = X + list(features_in) + list(features_out)\n",
    "        y = y + [0]*len(scores_in) + [1]*len(scores_out)\n",
    "        \n",
    "        if len(y) >= quantity_target:\n",
    "            break\n",
    "\n",
    "    np.save(f'./data/scores/scoresCIFAR10_SVHN_{len(y)}_{acc}',X)\n",
    "    np.save(f'./data/scores/labelsCIFAR10_SVHN_{len(y)}_{acc}',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracies to use for the plot. They must correspond to the files generated and saved with the previous cell.\n",
    "accuracies = [5733, 6061, 6530, 6929, 7502, 8064, 8587, 8944, 9184]\n",
    "\n",
    "roc_aucs = []\n",
    "\n",
    "for acc in tqdm(accuracies):\n",
    "    \n",
    "    X = np.load(f\"./data/scores/scoresCIFAR10_SVHN_10000_{acc}.npy\")\n",
    "    y = np.load(f\"./data/scores/labelsCIFAR10_SVHN_10000_{acc}.npy\")\n",
    "    \n",
    "    for k in range(17):\n",
    "        X[:,k] = X[:,k]/max(X[:,k].max(), -X[:,k].min()) # Normalizing Mahalanobis scores\n",
    "\n",
    "    y = 1-y # Inverting ID/OOD convention so that the LR's coefficients are positive\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.array(X),y, test_size=0.5, shuffle=True)\n",
    "    \n",
    "    LR = LogisticRegression(max_iter=10000, verbose=0, fit_intercept=False)\n",
    "    LR.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = LR.predict_proba(X_test)[:,1]\n",
    "    scores_ID, scores_OOD = [], []\n",
    "    \n",
    "    # Converting results to the format used in the metrics_eval function\n",
    "    for k in range(len(y_test)):\n",
    "        if not y_test[k]:\n",
    "            scores_OOD.append(y_pred[k])\n",
    "        else:\n",
    "            scores_ID.append(y_pred[k])\n",
    "            \n",
    "    roc_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(9*0.7,6*0.7), dpi=180)\n",
    "X_plot = np.array(accuracies)/100\n",
    "Y_plot = np.array(roc_aucs)*100\n",
    "plt.plot(X_plot,Y_plot, 'bs')\n",
    "plt.plot(X_plot,Y_plot)\n",
    "plt.xlabel(\"Base model validation accuracy (%)\")\n",
    "plt.ylabel(\"Mahalanobis OOD detector AUROC (%)\")\n",
    "plt.title(\"Influence of the Base Model's Performance on OOD detector\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
